#### ABOUT
--------------------------------------------
_This repository contains files related to pyspark that i went through learnings. I have included pretty much
everything from the Apache Spark 3.5.1 documentation(since there is no documentation for 3.1.2 as such which i have used in this project) prespective. Iam currerntly actively working on this repo so expect some changes every now and then_


#### TABLE OF CONTENT
  1. _Requirements_
  2. _Recommended modules_
  3. _Installation_
  4. _Configuration_
  5. _Troubleshooting & FAQ_
  6. _Maintainers_


##### 1.REQUIREMENTS
------------------------------------------------------------
_This module require the understanding and hands on knowledge of the below._

_1. python - a very basic understanding and working knowledge would be helpful. pyspark is written using python as a language and spark as the framework_

_2. SQL - a basic understanding of SQL concepts such as where clause, group by, window functions, CTE woud be enough._

_3. Theoretical understanding of spark - you can read and implement side by side._

##### 2.RECOMMENDED MODULES
##### 3.INSTALLATIONS
_This module requires the below installations for it to work on windows based laptops/PC_

|**Name**|**Version**|**Link**|
|----|--------|---------------|
|_python_|3.8.0|[Python](https://www.python.org/downloads/release/python-380/)
|_pyspark_|3.1.2|use command `pip install pyspark==3.1.2`
|_spark cluster_|3.1.2|[Spark](https://archive.apache.org/dist/spark/spark-3.1.2/)
|_winutils_|3.1.2|

##### 4.CONFIGURATIONS
##### 5.TROUBLESHOOTING AND FAQ
##### 6.MAINTAINERS









things to keep in mind
----------------------
1. incase you are unable to write it into sinks try to download hadoop.dll(for hadoop 3.1.2) and save it to C:/Windows/System32 